{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to combine Pytorch-Transformers (updated from Pytorch-pretrained-bert) with Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_train import BasicLearner\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "config = Config(\n",
    "    testing=False,\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    max_lr=3e-5,\n",
    "    epochs=1,\n",
    "    use_fp16= True,\n",
    "    bs=64,\n",
    "    discriminative=False,\n",
    "    max_seq_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the pytorch-pretrained-bert package, so install it if you do not have it yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT requires a special wordpiece tokenizer and a vocabulary to go along with that. Thankfully, the pytorch-pretrained-bert package provides all of that within the handy `BertTokenizer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_transformers import *\n",
    "\n",
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    config.bert_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAI has its own conventions for handling tokenization, so we'll need to wrap the tokenizer within a different class. This is a bit confusing but shouldn't be that much of a hassle.\n",
    "\n",
    "Notice we add the \\[CLS] and \\[SEP] special tokens to the start and end of the sequence here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly confusingly, we further need to wrap the tokenizer above in a `Tokenizer` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make sure fastai uses the same mapping from wordpiece to integer as BERT originally did. Again, fastai has its own conventions on vocabulary so we'll be passing the vocabulary internal to the `BertTokenizer` and constructing a fastai `Vocab` object to use for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
    "DATA_PATH = Path(\"./data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pieces we need to make BERT work with fastai! We'll load the data into dataframes and construct a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv(DATA_PATH/\"toxic_comment.csv\")\n",
    "\n",
    "# Use a smaller dataset to test\n",
    "#DATA = pd.read_csv(DATA_PATH/\"toxic_comment.csv\")[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_test = pd.read_csv(DATA_PATH/\"toxic_test.csv\")\n",
    "train, val = train_test_split(DATA)\n",
    "\n",
    "if config.testing:\n",
    "    train = train.head(1024)\n",
    "    val = val.head(1024)\n",
    "    test = test.head(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build the databunch using the tokenizer and vocabulary we build above. Notice we're passing the `include_bos=False` and `include_eos=False` options. This is to prevent fastai from adding its own SOS/EOS tokens that will interfere with BERT's SOS/EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = TextDataBunch.from_df(\".\", train, val, DATA_test,\n",
    "                  tokenizer=fastai_tokenizer,\n",
    "                  vocab=fastai_bert_vocab,\n",
    "                  include_bos=False,\n",
    "                  include_eos=False,\n",
    "                  text_cols=\"comment_text\",\n",
    "                  label_cols=label_cols,\n",
    "                  bs=config.bs,\n",
    "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUNCH = Path(DATA_PATH/\"databunch_full\")\n",
    "DATA_BUNCH.mkdir(exist_ok = True)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch.save(DATA_BUNCH/\"data_save.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = load_data(DATA_BUNCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (7500 items)\n",
       "x: TextList\n",
       "[CLS] what ? who changed again it ? i fixed several times . please stop van ##dal ##ising this page ! ! [SEP],[CLS] expanding on the legal ##ity of step ##mi ##x is step ##mi ##x really legal ? the small inclusion into the article makes it appear all ok ##ey do ##key , but with songs from dd ##r and such , is it really all that legal ? [SEP],[CLS] \" je ##z ##ta ##h , the explain ##ation above ( that since the article of the 1982 war is called 1982 lebanon war and since that war only since 2006 has been called the first lebanon war only in and to israel , then this war should be called 2006 lebanon war is incorrect and that logic is not supported by wikipedia policy . the 2006 lebanon has very little credentials ( as i am proving above . ) israel - hezbollah war carries no po ##v implications ( the excuse george . sal ##iba named above is purely original and cannot be heard anywhere else in all seriousness . ) using his logic , 2006 lebanon war would be incorrect since fighting began and [SEP],[CLS] \" where tr ##m posted his sl ##ur against me is completely irrelevant . a personal attack made behind my back is still a personal attack . tr ##m [ repeated his attack on me with the exact phrase ( \" \" i . e . paranoia \" \" ) that you explicitly warned him was \" \" not acceptable at all \" \" ( i . e . block ##able offence ) . with your \" \" location matters \" \" logic , all i would ' ve had to do is post my reply on any talk ##page except for w ##t : d ##yk . seriously ? — \" [SEP],[CLS] you ask me not to post on your page and the you go and post on mine . keep your pe ##do ##fi ##le self , off of my page . go back to your child - love talking points , and kidd ##ie porn . [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,toxic\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (2500 items)\n",
       "x: TextList\n",
       "[CLS] \" oppose i find it hard to believe this requires an rfc , but such is the wi ##ki ##world . . . [UNK] [UNK] 見 学 / [UNK] [UNK] \" [SEP],[CLS] \" hello deacon , thanks for the kind words . your word ##ing is better regarding territory , but i think that it still might specifically mention the firth of clyde , since north ##umb ##ria ruled the area south of it for a while . . . how about \" \" between the hum ##ber and the firth ##s of clyde and forth \" \" , or similar ? but there ' s no great preference here either way . os ##wi ##u chose the roman way at w ##hit ##by in 66 ##4 , and he died in 670 ; thereafter there was a steady erosion of iona - based cleric ##s in north ##umb ##rian territory , quite noticeable by pe ##ht ##helm [SEP],[CLS] \" december 2009 please do not add inappropriate external links , as you did with this edit to heath ( habitat ) . if you continue to do so , you will be blocked from editing . \" [SEP],[CLS] please do not add links back to your site - w ##p : el is very clear on this . you need to suggest the link on the talk ##page and other editors will then decide if it should be included . i will remove any attempts to re - add the link in accordance with policy . [SEP],[CLS] stuff like that is often a remnant of ye old ##en days of wikipedia , when there was far fewer rules and far , far fewer people roaming about looking for opportunities to enforce them . [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (1000 items)\n",
       "x: TextList\n",
       "[CLS] yo bitch ja rule is more su ##cc ##es ##ful then you ' ll ever be what ##s up with you and hating you sad mo ##fu ##ck ##as . . . i should bitch slap ur pet ##hed ##ic white faces and get you to kiss my ass you guys sick ##en me . ja rule is about pride in da music man . don ##t di ##ss that shit on him . and nothin is wrong bei ##n like tu ##pac he was a brother too . . . fuck ##in white boys get things right next time . , [SEP],[CLS] = = from rfc = = the title is fine as it is , im ##o . [SEP],[CLS] \" = = sources = = * za ##we ashton on lap ##land — / \" [SEP],[CLS] : if you have a look back at the source , the information i updated was the correct form . i can only guess the source hadn ' t updated . i shall update the information once again but thank you for your message . [SEP],[CLS] i don ' t anonymous ##ly edit articles at all . [SEP]\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the data in place, we will prepare the model and loss functions. Again, the pytorch-pretrained-bert package gives us a sequence classifier based on BERT straight out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n",
    "bert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multilabel classification problem, we're using `BCEWithLogitsLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more recent version fastai implemented exactly this version thus you could just directly call a partial funtion\n",
    "# accuracy_thresh\n",
    "def accuracy_thresh2(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True)->Rank0Tensor:\n",
    "    \"Computes accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "    return ((y_pred>thresh).byte()==y_true.byte()).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_02 = partial(accuracy_thresh2, thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch_bert(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,\n",
    "               cb_handler:Optional[CallbackHandler]=None)->Tuple[Union[Tensor,int,float,str]]:\n",
    "    \"Calculate loss and metrics for a batch, call out to callbacks as necessary.\"\n",
    "    cb_handler = ifnone(cb_handler, CallbackHandler())\n",
    "    if not is_listy(xb): xb = [xb]\n",
    "    if not is_listy(yb): yb = [yb]\n",
    "    out = model(*xb)\n",
    "    out = out[0]\n",
    "    out = cb_handler.on_loss_begin(out)\n",
    "\n",
    "    if not loss_func: return to_detach(out), yb[0].detach()\n",
    "    loss = loss_func(out, *yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss,skip_bwd = cb_handler.on_backward_begin(loss)\n",
    "        if not skip_bwd:                     loss.backward()\n",
    "        if not cb_handler.on_backward_end(): opt.step()\n",
    "        if not cb_handler.on_step_end():     opt.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change the loss_batch function in the loaded fastai module\n",
    "module_basic_train = sys.modules['fastai.basic_train']\n",
    "module_basic_train.loss_batch = loss_batch_bert\n",
    "sys.modules['fastai.basic_train'] = module_basic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can build the `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(\n",
    "    databunch, bert_model,\n",
    "    loss_func=loss_func, metrics = acc_02\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! All the rest is fastai magic. For example, you can use half-precision training simply by calling `learner.to_fp16()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_fp16: learner = learner.to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the learning rate finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VFX6x/HPk0YSSAgloYWmEBCQGlFEQey6LthQsWBndUUsu+667uruT13dpq6FVRF7Y7FjA9cuVUKVqkgxkRJKIAHSc35/zCRGCCRAbu5M8n2/XvNy5s6Zuc9xwjxz7rn3OeacQ0REBCDC7wBERCR0KCmIiEgFJQUREamgpCAiIhWUFEREpIKSgoiIVFBSEBGRCkoKIiJSQUlBREQqRPkdwIFq2bKl69Spk99hiIiElXnz5m1xziVX1y7skkKnTp3IyMjwOwwRkbBiZutq0k6Hj0REpIKSgoiIVFBSEBGRCkoKIiJSQUlBREQqKCmIiEgFJQUREamgpCAiEmIKS0p5ec468otK63zfSgoiIiFmzupt/PGtJdzz/rI637eSgohIiNmeXwzAK3N+YOqSjXW6byUFEZEQk1cQSAodW8Rz+5uL2bijoM72raQgIhJicvNLABh/cX8Ki8u4dfJCyspcnexbSUFEJMTkFRQTFWH0bJvIX4b3YOb3W5nw1eo62beSgohIiMktKCYhNgoz44L09px5ZGv+NW0li7O2e75vJQURkRCTV1BCYlw0AGbG/ef0pnPLxqzfnu/5vsNuPQURkfouNz8wUijXND6aqTcPITLCPN+3RgoiIiEmr6CExNjon22ri4QASgoiIiGnfE7BD0oKIiIhpqqRQl1RUhARCTGBOQUlBRGRBq+ktIxdRaUkxunwkYhIg7ezMHA1s0YKIiJCXkF5UtBIQUSkwdsRrJCqiWYREakYKSRqpCAiIuVls8vLXNQ1JQURkRCSqzkFEREpVzFS0JyCiIiUL7DTRCMFERHJKygmPiaS6Eh/vp6VFEREQoifxfDA46RgZqeb2UozW2Vmt1fxfAcz+8zMFpjZYjM708t4RERCnZ/F8MDDpGBmkcB44AygBzDKzHrs0exPwGTnXD/gIuA/XsUjIhIO6vNIYSCwyjm32jlXBEwCRuzRxgGJwftNgfUexiMiEvIqL8XpBy+TQjsgs9LjrOC2yv4CXGpmWcAHwI0exiMiEvL8LJsN3iaFqtaOc3s8HgU855xLBc4EXjSzvWIyszFmlmFmGZs3b/YgVBGR0BCYU6ifh4+ygPaVHqey9+Ghq4HJAM65WUAs0HLPN3LOTXDOpTvn0pOTkz0KV0TEX8654JxC/RwpzAW6mllnM4shMJE8ZY82PwAnAZjZEQSSgoYCItIgFZaUUVzqfFtgBzxMCs65EmAsMA1YTuAso6VmdreZDQ82+w1wrZktAl4FrnDO7XmIqVaUlTk8emsRkVqRGyyb7edIwdN05Jz7gMAEcuVtd1W6vwwY7GUM5Z6ZsYZ/TF1JYlwUibHRJMRFk9wkhrP7teP0nq2J8unqQRGRcrk+l80Gj5NCKOmdmsRVx3Umt6CY3PxicgtKWLExj7GvLKBN01hGD+rEqIHtSYqP8TtUEWmgcn0uhgcNKCkM7NycgZ2b/2xbWZnjs5XZPDNjDX+fuoJ/f/wtnVs2JiUxllYJjWjdNJZTerSid2qST1GLSEPi91Kc0ICSQlUiIoyTjmjFSUe0YsXGXF7LyOKHbbvJzi1g5cZcNucV8uinq/hln7bcdmo3OrSI9ztkEanHyucU/Lx4rUEnhcq6t07kzrN+XoUjr6CYp75czVNfrWHqkg1cdkwnRvRtS4QZLnjJRevEWFISY/0IWUTqGY0UQlxCbDS3ntqNS47pyL8//pbnZq7hmRlrftYmOtK46rjOjB3WxdczBkQk/GlOIUy0Sozl/nN7M2bI4XyfvRMAM3AOpi3dyJNfrOaNeT/yu9O6cf6AVCIiqrqYW0Rk//IKiomMMOJjIn2LQUnhAHRu2ZjOLRv/bNvJPVpx2aCO/N+7y/jdG4t59LPvaNG4EVERRlSkER0ZQcsmjUhOaERy8L9tmsbSNimO1k1jfVtIQ0RCT15BCQmxUZj598NSSaEW9E5N4vXrBjFl0XreXbSBotIySkrLKClz5BaUsGbLLrLzCikqKfvZ6yIsMArp3jqB9E7NGdCxGX1Sk4jz8VeCiPgnUAzP369lJYVaYmaM6NuOEX33LAQb4Jwjr7CE7NxCNuzIZ/32fH7MyScrJ5/FP+7gs5UrAYiKMPp3bMZZvdtweq/WpCRoElukofB7gR1QUqgzZkZibDSJsdF0SWmy1/M5u4qY/0MOGety+HjZJu56Zyl/mbKUozu34PwBqZzTr53mKkTqOb8X2AElhZDRrHFMxTUTvz+9O99uyuO9xRt4b/F6fvPaIl6YvY57R/TiyNSmfocqIh7JKyihQ3N/r4fSLGeISmuVwK2npPHJrUP594V9+TEnn+Hjp/Ont79hx+5iv8MTEQ/4vcAOaKQQ8syMs/u148QjUnjwo295YdZa3lm4nmHdUjixewpD0pJp3lj1mkTqg8BSnDp8JDWQGBvNX4b3ZGR6Ks9MX8vnK7OZsmg9EQb9OzTjjl8cQf8OzfwOU0QOUmlZ4GQUjRTkgPRs25QHLuhDWZkLnLW0IpvXMjIZ+cQsbjyxC2OHdVEZcJEwtLPQ/7LZoDmFsBURYfRtn8Qtp6Qx9ZYhDO/Tln9//B0jn5zFuq27/A5PRA5QRTE8n0cKSgr1QGJsNA9d2JdHRvVjVfZOznz4KyZ+tXqvi+VEJHSVF8Pze05BSaEeGd6nLVNvHkJ6p+bc+/5yTn3oC6Yu2ahlSEXCQHkxPL/nFJQU6pl2SXE8f9VAnrvyKKIjI7jupXlc+ORsVm7M8zs0EdmPipGCkoJ44YRuKXx40/Hce3YvVm3eydnjZzBl0Xq/wxKRfSifU/D7imYlhXosKjKCS4/pyNSbjqdn20TGvbqA+z5YTkmp5hpEQk1egZKC1JGUxFheufYYLjumIxO+XM3lz37Ntl1FfoclIpXkVqy6psNHUgdioiK45+xe/OP83sxdm8OpD33JOwt/1CS0SIjIKygmNjqCmCh/v5aVFBqYC9Lb89avj6VtUiw3TVrIZU9/zZotuq5BxG+5+f6XzQYlhQapZ9umvPXrwdwzoieLMrdz2r+/5JFPvqNYcw0ivskr9L9sNigpNFiREcZlgzrxyW+GclrP1jz4v28Z+cQs1mrUIOKL3PwSEuM0UhCfpSTG8uiofjx2cT9Wb97JmY98xX/n/qC5BpE6llfgf9lsUFKQoLN6B66G7pOaxO/f+IbrX5pfcYWliHgvsBSnDh9JCGmbFMfL1xzNH87ozsfLN3H2+Bms3rzT77BEGoRcjRQkFEVEGL8aejgvXXM023cXM2L8DD5bme13WCL1Xm4ILLADSgqyD8cc1oJ3bhhMarN4rnpuLk988b3mGUQ8UlBcSlFJmU5JldDWvnk8b1w/iDN7teFvH67g8mfnsnFHgd9hidQ7PxXDq+cjBTM73cxWmtkqM7u9iucfMrOFwdu3Zrbdy3jkwMXHRPHYxf24e0RPvl6zlVMf+oK3F+hKaJHaFCpls8HDpGBmkcB44AygBzDKzHpUbuOcu8U519c51xd4FHjTq3jk4JkZowd14sObhtAlpQk3/3ch1780n607C/0OTaReCJUFdsDbkcJAYJVzbrVzrgiYBIzYT/tRwKsexiOHqHPLxrx23bHcfkZ3Pl2RzekPf8XnmoQWOWQ/lc2uxyMFoB2QWelxVnDbXsysI9AZ+NTDeKQWREYY1w09nLdvGEyz+GiueHYuf5mylILiUr9DEwlbobLADnibFKyKbfs6EH0R8LpzrspvFjMbY2YZZpaxefPmWgtQDl6PtolMGXscVw7uxHMz1zL8seks35Drd1giYSk3RNZSAG+TQhbQvtLjVGBfS39dxH4OHTnnJjjn0p1z6cnJybUYohyK2OhI/vzLnjx/1UBygtc0PD9zrSahRQ5Q+fomzeJjfI7E26QwF+hqZp3NLIbAF/+UPRuZWTegGTDLw1jEQ0PTkpl60/Ec16Ulf56ylGtfmEeOFvERqbHMbbtp2SSGuJhIv0PxLik450qAscA0YDkw2Tm31MzuNrPhlZqOAiY5/bwMay2aNOLpy9O566wefPntZs54+CvmrN7qd1giYSErJ592zeL9DgPw+DoF59wHzrk059zhzrm/Brfd5ZybUqnNX5xze13DIOHHzLjquM68+etjiY+J5JKJc3hlzg9+hyUS8jJzdtO+WZzfYQC6olk80KtdU94eO5jBXVpyx1vfcPe7yygt00BQpCqlZY712/NJbQgjBWm4EmOjefrydK4c3IlnZqzh6ufnkqdS3CJ72ZRbQHGpo31zjRSknouKjODPv+zJX8/pxVffbeG8x2eSlbPb77BEQkpWTj6ARgrScFxydEdeuGogG3YUcPb4mSzKVIkrkXKZ2wI/lDSnIA3K4C4tefP6Y4mNjuDCCbOYumSj3yGJhITykULbJCUFaWC6tkrgrV8PpnvrRK5/eR4TvtQaDSKZObtpldiI2Gj/r1EAJQWpY8kJjZg05hjO7NWG+z5YwW8mL1LdJGnQsnJ2h8x8AigpiA9ioyN5dFQ/bj0ljbcW/qgJaGnQMrflh8x8AigpiE8iIoxxJ3Xl6cvT+WHbbn756HRmrtrid1gidaqktIyNuQUaKYiUO7F7K6aMPY6WTRpx6dNzeH1elt8hidSZDTsKKC0LnWsUQElBQkDnlo15+4bAFdC3vb6I/85VaQxpGDKDh001UhDZQ+NGUTw1Op0hXZP5/Rvf8NLsdX6HJOK5rG2B01HbKymI7C02OpIJowdwYvcU/vT2Ep6fudbvkEQ8lZWzmwiDNkmxfodSQUlBQkqjqEieuHQAp/RoxZ+nLOXVr3UoSeqvzJx82jSNIzoydL6KQycSkaCYqAj+c0l/hqYlc+fbS7Qug9RbWTm7aRdCp6OCkoKEqOjICB4Z1Y8OLeK5/uX5FfVhROqTwDUKoTOfAEoKEsKaxkUzcXQ6xaVlXPtCBrsKS/wOSaTWFJaUsimvIKROR4UaJgUzO9zMGgXvn2Bm48wsydvQROCw5CY8dnF/vt2Ux62TF1KmxXqknli/vQDnQut0VKj5SOENoNTMugBPA52BVzyLSqSSoWnJ3HHmEUxbuolHPv3O73BEakV5aZdQKnEBNU8KZc65EuAc4N/OuVuANt6FJfJzVx/XmXP7t+PfH3/Hpys2+R2OyCHLDF6jkNo8PEcKxWY2CrgceC+4LdqbkET2Zmbcd86R9GiTyM2TFrJu6y6/QxI5JFk5u4mKMFonhs41ClDzpHAlMAj4q3NujZl1Bl7yLiyRvcVGR/LkZQMwM657aT75RSq5LeErMyeftklxREaY36H8TI2SgnNumXNunHPuVTNrBiQ45/7mcWwie2nfPJ5HRvVjxcZc7njrGy3SI2ErK2d3yJ15BDU/++hzM0s0s+bAIuBZM3vQ29BEqjY0LZlbT07jrQU/8sIs1UiS8JS5LZ/UpNCaT4CaHz5q6pzLBc4FnnXODQBO9i4skf27YVgXTj4ihXvfX8bCzO1+hyNyQAqKS9myszB8RwpAlJm1AS7gp4lmEd9ERBgPjOxLSkIsN7w8n+27i/wOSaTGskKwZHa5miaFu4FpwPfOublmdhigE8bFV03joxl/SX+y8wr47WuLNb8gYSMzJ1gyO1xHCs6515xzvZ1z1wcfr3bOnedtaCLV69s+iTvOPIKPl29i4ldr/A5HpEaytoX5SMHMUs3sLTPLNrNNZvaGmaV6HZxITVxxbCfO6NWav01dwbx12/wOR6RamTn5xERFkNykkd+h7KWmh4+eBaYAbYF2wLvBbSK+MzP+fn5v2iXFMfaVBeTs0vyChC7nHJ+uyKZ3u6ZEhNg1ClDzpJDsnHvWOVcSvD0HJHsYl8gBSYyN5j+X9GfrziJ++9oizS9IyJr/w3ZWZe9kZHpoHmypaVLYYmaXmllk8HYpoJVPJKT0ateUO87szicrsnl6uuYXJDRNnptJfEwkv+jd1u9QqlTTpHAVgdNRNwIbgPMJlL7YLzM73cxWmtkqM7t9H20uMLNlZrbUzFR5VQ7J5cd24tQerfj71BW6fkFCzq7CEt5bvJ6zerehSaMov8OpUk3PPvrBOTfcOZfsnEtxzp1N4EK2fTKzSGA8cAbQAxhlZj32aNMV+AMw2DnXE7j5YDohUs7M+Of5fUhJiOXGV+ezI7/Y75BEKry/eAO7ikq58Kj2foeyT4ey8tqt1Tw/EFgVPH21CJgEjNijzbXAeOdcDoBzLvsQ4hEBAtcvPHpxPzZsL+D2N3T9goSO/2ZkclhyY/p3aOZ3KPt0KEmhumnzdkBmpcdZwW2VpQFpZjbDzGab2emHEI9Ihf4dmnHbad34cMlGHvt0ld/hiLAqO49563K4ML09ZqF31lG5QzmoVd3Pr6p6vedrooCuwAlAKvCVmfVyzv3sYLCZjQHGAHTo0OGggpWGZ8yQw1i5MY8H/vctHVrEM6Lvnr9JROrO5IwsoiKMc/uH5llH5fabFMwsj6q//A2o7vrsLKDygbNUYH0VbWY754qBNWa2kkCSmFu5kXNuAjABID09XccCpEbMjPvPO5Ift+dz22uLadM0joGdm/sdljRAxaVlvDk/ixO7p5CcEHoXrFW238NHzrkE51xiFbcE51x1o4y5QFcz62xmMcBFBC6Aq+xtYBiAmbUkcDhp9cF1RWRvjaICC/OkNotjzIsZrNmiFduk7n26IpstO4tCeoK53KHMKexXcE3nsQQK6S0HJjvnlprZ3WY2PNhsGrDVzJYBnwG3Oed0/YPUqqT4GJ698igizLjy2a9VUVXqVGFJKRO+XE1KQiOGpoX+Nb8WbmdmpKenu4yMDL/DkDA0b902Lpowm9N6tuaxi/v7HY40AMWlZdzw8nw+WraJB0b24bwB/s0nmNk851x6de08GymIhJoBHZtz88lpvLd4A1MW7Tm9JVK7Ssscv5m8iI+WbeL/hvf0NSEcCCUFaVB+NeQw+nVI4k9vfcPGHQV+hyP1VFmZ4w9vLmbKovX8/vTuXH5sJ79DqjElBWlQoiIjePCCvhSXOm57XYXzxBv3vr+cyRlZjDuxC9efcLjf4RwQJQVpcDq3bMwdvziCr77bwouz1/kdjtQzCzO388yMNYwe1JFbTknzO5wDpqQgDdKlR3dgaFoy932wnNWbd/odjtQj/5q2kuaNY/jd6d1D+srlfVFSkAbJzPjH+b1pFBXJ715fTFmZDiP5rbi0jAuemMVFE2Yx4cvvWZWdF3aH92au2sL0VVv49QmHh2wV1OooKUiD1SoxljvP6kHGuhxemLXW73AavCkL1/P12m1s2FHAfR+s4OQHv2ToPz9n8tzM6l8cApxz/POjlbRpGsulx3T0O5yDpqQgDdp5/dsxNC2Zf0xbSWZwMXWpe2Vljie++J7urRP4/LcnMOP2E7n37F60aBLD7W8u5stvN/sdYrU+WZ7Ngh+2M+6krsRGR/odzkFTUpAGzcy479wjMeAPb34Tdocr6otPV2TzXfZOrht6OGZGu6Q4Lj2mIy9fczRprRK48dUFIZ20y8oc//poJZ1axHN+mFyPsC9KCtLgtUuK4/Yzj2D6qi28lpHldzj1xuKs7dz59hIKikurbfv4F9/TLimOs3q3+dn2+JgonrxsAM45xrw4j/yi6t/LD+8uXs+KjXncckoa0ZHh/bUa3tGL1JJLBnZgYOfm3PP+Mjbl6qK22vDR0k28OHsdt05euN+J/LlrtzFvXQ5jhhxGVBVfqB1bNObhUf1YsTGXP7wZeosmbdiRzwMffUv31gn8MkTXXT4QSgoiQESE8ffzelNUUsY1z2eQrcRwyMqXQv3gm438feqKfbZ7/PPvad44hgvS911BdFi3FH5zShpvL1zP09PX1HqsB6OwpJTxn63ixH99wcbcAu48qwcREeF3CuqewvOcKREPdG7ZmP9c0p8bX13AiPEzmHh5Oj3bNvU7rLCVW1BMxxbxDOmazJNfria1eTyX7XFWzoqNuXy6IptbT0kjLmb/k7O/PqEL3/y4g/s+WE6H5vGc2rP1AcdUWFLKFys30zs1idZNY2v8mlnfbyUywoiLjiQ2OpKsnHz+9uFy1m7dzWk9W/GnX/SgffP4A44nFCkpiFRy0hGteO26QVzzfAYjn5jFIxf14+QerfwOKyztyC+maVw0f/5lD37cns+f31lCalIcw7qnVLR58ovVxMdEMnpQ9adwRkQYD13Yl1ETZjNu0gJeufaYGq91nJWzm1fm/MDkjEy27CyiffM4XvvVsdUmBucc415dwLSlm/Z67vDkxrx49UCO7xr65bAPhEpni1QhO7eAa17I4Jsfd/B/w3syelAnv0MKO+f+ZwbxMVG8dM3R7Cos4YInZ7FiYx7xMZGUlDpKysooLnVcfVxn7jyrR43fd8vOQs79z0x2FpbwxvXH0rll44rn1mzZxUdLN7I9v5hdhSXsLCxhU24Bs74PLNNy0hGtOLF7Cn99fzmtEhsx+VeDaNFk3yuhvTBrLXe9s5SbTurK8V1bkl9cSn5RKRFmDElLJiYqfI7A17R0tpKCyD7kF5Vy46sL+GTFJl6++miO7dLS75DCykkPfE731omMvySwdkV2XgFPT19DUUkZ0ZERFYdjRg/qSFJ8zAG995otuzjv8Zk0aRTF69cNYlHWDl6YtZavvtsCQHSk0bhRFI1jokiIjeLkI1ox6ugOtEsKrCL89ZptjH5mDoe1bMKrY46haVz0XvtYun4H54yfyXFdW/L05elhWbKiMiUFkVqwu6iE4Y/NYPvuYj646ThSEmp2HFog/d6POaVHCvef29uT91/wQw6jnpodHHU4WifGcvHRHbjoqPakJFb/OX3x7WaueX4uR7ZryotXH03jSmUpdhWW8MtHp7OrqIQPbxpC88YHlrRCkRbZEakF8TFRjL+4PzsLi7l50kJKVSOpxnILikms4hd4benXoRmPXzqAE7un8Pgl/Zn++2GMO6lrjRICwNC0ZB4d1Y+Fmds59aEvufe9Zcxbt42yMsed7yxh7dZdPHxRv3qREA6EJppFqtGtdQJ3D+/F795YzKOffsfNJ4dfOeS6VlBcSlFJGYmx3iUFCJyqOqxbSvUN9+H0Xm145oqjeH7mWp6ftZaJ09fQonEMW3cVcdNJXTnmsBa1F2yYUFIQqYGR6anMXr2Vhz/5joGdmmt+oRq5wWsUqjpWH2pO6JbCCd1SyC0o5tPl2Xy4ZANx0ZGMO6mr36H5QklBpAbMjHvO7sWirO2Mm7SQj26pH8eZvVJ+4ZqXh49qW2JsNGf3a8fZ/dr5HYqvNKcgUkONG0Xx6Kj+7Mgv4q53lvgdTkjLLQifkYL8nJKCyAHo0TaRm09O473FG3h30Xq/wwlZO8Lo8JH8nJKCyAH61ZDD6NM+iTvfWUJ2nmokVSU3vwSAxFgdoQ43SgoiBygqMoIHRvYhv6iUP7yhNRiqopFC+FJSEDkIXVKacNtp3fhkRTavz9MaDHvKDcOJZglQUhA5SFcN7szAzs25+91lfLspz+9wQsqO/GLiYyLDfsGZhkifmMhBiogwHrygD3ExkVw6cQ7rtu7yO6SQkVtQ7PmFa+INJQWRQ5DaLJ6Xrjma4tIyLn5qDuu35/sdUkgoL5st4UdJQeQQpbVK4IWrjiY3v5hLJ85hy85Cv0Py3Y78YhLjdOZROFJSEKkFR6Y25Zkrj2L9jnwunTiH7buL/A7JV7n5JRophCklBZFaclSn5ky4LJ3Vm3dxSQNPDDvyNacQrjxNCmZ2upmtNLNVZnZ7Fc9fYWabzWxh8HaNl/GIeG1IWjJPjh7Ad9k7ufipOeTsapiJweuy2eIdz5KCmUUC44EzgB7AKDOras29/zrn+gZvE72KR6SuDOuWwoTLBrBq804unjiHbQ0sMZSWOfIKSpQUwpSXI4WBwCrn3GrnXBEwCRjh4f5EQsYJ3VKYODqd1Zt3cvFTsxtUYthZEChxoTmF8ORlUmgHZFZ6nBXctqfzzGyxmb1uZu09jEekTg1JS+bpy49izZZd3PDy/AazaltF2WzVPQpLXiaFqla53vNfxbtAJ+dcb+Bj4Pkq38hsjJllmFnG5s2bazlMEe8c17Ul957di1mrt/Lwx9/6HU6dUNns8OZlUsgCKv/yTwV+VmvYObfVOVd+UvdTwICq3sg5N8E5l+6cS09OTvYkWBGvjExvz/kDUnn0s1V8+W39/1GjYnjhzcukMBfoamadzSwGuAiYUrmBmbWp9HA4sNzDeER8c8+IXqSlJHDzfxeycUf9LretYnjhzbOk4JwrAcYC0wh82U92zi01s7vNbHiw2TgzW2pmi4BxwBVexSPip7iYSMZf0p+C4lJufHU+xaVlfofkGY0Uwpun1yk45z5wzqU55w53zv01uO0u59yU4P0/OOd6Ouf6OOeGOedWeBmPiJ+6pDTh/nOPZO7aHP7+Yf39Uy+fU9BIITzp9ACROjSibzvmr8th4vQ1HJbchIuP7uB3SLVuR34xkRFG45hIv0ORg6CkIFLH7jyrB+u27ebOd5aQ2iyOIWn16+SJ3PwSEmOjMKvqBEQJdap9JFLHoiIjeHRUP7qmNOGGl+ezcmP9WqBHZbPDm5KCiA8SYqN55oqjiIuJ5Krn5rI5r/6U2w6UzVZSCFdKCiI+aZsUx9OXH8W2XUVc+0IGBcWlfodUK3ILNFIIZ0oKIj46MrUpD13Yl4WZ2/njW0twLvxLYahsdnhTUhDx2em9WnPzyV15Y34WT09f43c4hyw3XxVSw5mSgkgIGHdiV07r2Yr7PljOV9+FbykM5xy5WoozrCkpiISAiAjjwQv60jUlgbGvLGDtll1+h3RQCkvKKCot05xCGFNSEAkRjRtF8dTodMzgmhcy2Loz/M5I+qlstpJCuFJSEAkhHVrE8/glA8jctptLwnDVtlzVPQp7SgoiIWbQ4S2YeHk6q7fs4tKJ4bXOs4rhhT8lBZEQdHzXZJ4anc6qzTu59Ok5bN90V8KWAAANDElEQVQdHolBxfDCn5KCSIgampbMk5cN4LtNgcSwKTf012HQSCH8KSmIhLBh3VJ48rIBfJ+9i1888lXIn66am18CaH3mcKakIBLihnVPYcrYwTSLj2H0M1/z4EcrKS0LzSufd2jVtbCnpCASBrq2SuCdsYM5v38qj3y6iksmzg7JInq5+cXEx0QSHamvlnClT04kTMTHRPHPkX3418g+LMzcztnjZ4Rc2W2VzQ5/SgoiYeb8AalM/tUgikvLOO/xmXy+MtvvkCqoGF74U1IQCUO9U5N4Z+xg2jeP56rn5vLCrLV+hwSobHZ9oKQgEqbaNI3j9esGMaxbCne9s5R73ltGmc8T0DvyS1QML8wpKYiEscaNopgwOp0rju3E09PXcOOkBRSW+LdYT65WXQt7SukiYS4ywvjzL3vQpmks93+4gq07C5kwOt2XY/u5mlMIexopiNQDZsavhh7OQxf2IWNtDhc8MYuNO+r2CujSMkdeYYnmFMKckoJIPXJOv1SevfIoMrftZuSTM8nctrvO9p2nukf1gpKCSD1zfNdkXrn2GHbsLubCJ2fV2YI95SUuNFIIb0oKIvVQn/ZJvDrmGPKLS7lwwixWZe/0fJ8/LbCjqcpwpqQgUk/1bNuUSWMGUVrmuGjCbM+vfi4vm62RQnhTUhCpx7q1TmDSmEFEGIx8YiZffOtdldWKstnxSgrhTElBpJ7rktKEN64/lrZJcVz57Nc8+cX3OFf7F7nlan3mekFJQaQBaN88njd/fSxn9GrD/R+u4Ob/LiS/qHYvctMCO/WDkoJIAxEfE8VjF/fjttO6MWXRes59fCbL1ufW2vvvyC8mMsKIj4mstfeUuqekINKAmBk3DOvC05enszmvgOGPTeeBj1bWSmmM8mJ4ZlYLkYpfPE0KZna6ma00s1Vmdvt+2p1vZs7M0r2MR0QCTuzeiv/dMpThfdry6Ker+MUj05m3btshvee6rbtJ0iRz2PMsKZhZJDAeOAPoAYwysx5VtEsAxgFzvIpFRPbWrHEMD17Yl+euPIr8olLOf2IWv399MVt2HviKbqs372T6qi2cdWQbDyKVuuTlSGEgsMo5t9o5VwRMAkZU0e4e4B9A3RZqEREATuiWwrRbhnDNcZ15Y34Ww/71OU9PX0NxaVmN3+PZGWuJjojgskGdvAtU6oSXSaEdkFnpcVZwWwUz6we0d869t783MrMxZpZhZhmbN3t3nrVIQ9WkURR//EUPpt48hH4dmnHPe8s44+GvmLN6a7WvzdlVxGvzMjm7X1uSExrVQbTiJS+TQlWzTRUnR5tZBPAQ8Jvq3sg5N8E5l+6cS09OTq7FEEWksi4pTXj+yqOYODqdwpJSLpwwmzve+qbiauWqvPL1DxQUl3H1cYfVYaTiFS+TQhbQvtLjVGB9pccJQC/gczNbCxwDTNFks4i/zIyTe7Ri2s1DGDPkMCZ9/QMnP/AF05Zu3KttYUkpz81cy/FdW9KtdYIP0Upt8zIpzAW6mllnM4sBLgKmlD/pnNvhnGvpnOvknOsEzAaGO+cyPIxJRGooPiaKO848grdvGEyLJo341YvzuPW/C9ldVFLR5r1FG9icV8g1x2uUUF94lhSccyXAWGAasByY7JxbamZ3m9lwr/YrIrWrd2oSU8YO5qaTuvLWwh8Z8dgMvtuUh3OOidPXkNaqCUO6tvQ7TKkl5kUNFC+lp6e7jAwNJkT8MGPVFm6atIBdhaWMGtiBZ2as4R/n9eaCo9pX/2LxlZnNc85Ve3heVzSLSI0N7tKS98cdz5HtmvLMjDW0bBLD8L5t/Q5LapFWwxCRA9IqMZZXrj2aZ2asoXPLJsRGq9ZRfaKkICIHLCoygjFDDvc7DPGADh+JiEgFJQUREamgpCAiIhWUFEREpIKSgoiIVFBSEBGRCkoKIiJSQUlBREQqhF3tIzPbDKzbY3NTYEc12/b3uPx+5W0tgS0HGWZV8RxIu1Drz/5irUmb2upP5fuh3p89t6k/B+ZQ+rOv5w70b6zy/frQn47OueoXpHHOhf0NmFDdtv09Lr+/x7aM2oznQNqFWn9q2iev+7NH30K6PzXpg/rjTX9q2qeG2J+a3OrL4aN3a7Btf4/f3Ueb2oznQNqFWn9q+l5e96emcdSE1/3Zc5v6c2AOpT/7eu5g/sbqW3+qFXaHj+qKmWW4GpSZDRfqT2hTf0JbfevP/tSXkYIXJvgdQC1Tf0Kb+hPa6lt/9kkjBRERqaCRgoiIVKj3ScHMnjGzbDNbchCvHWBm35jZKjN7xMys0nM3mtlKM1tqZv+o3airjavW+2RmfzGzH81sYfB2Zu1Hvs+YPPmMgs//1sycmdXZIsIefT73mNni4GfzkZnV2XJnHvXnn2a2Itint8wsqfYj32dMXvRnZPC7oMzMwnvu4VBOswqHGzAE6A8sOYjXfg0MAgz4EDgjuH0Y8DHQKPg4pR706S/Ab+vLZxR8rj0wjcB1LS3DuT9AYqU244Anwrw/pwJRwft/B/4e5v05AugGfA6k11VfvLjV+5GCc+5LYFvlbWZ2uJlNNbN5ZvaVmXXf83Vm1obAP8RZLvCpvwCcHXz6euBvzrnC4D6yve3Fz3nUJ9942J+HgN8BdTpx5kV/nHO5lZo2pg775FF/PnLOlQSbzgZSve3FTzzqz3Ln3Mq6iN9r9T4p7MME4Ebn3ADgt8B/qmjTDsiq9DgruA0gDTjezOaY2RdmdpSn0dbMofYJYGxwOP+MmTXzLtQaOaT+mNlw4Efn3CKvA62hQ/58zOyvZpYJXALc5WGsNVEbf2/lriLwq9tPtdmfsNbg1mg2sybAscBrlQ4/N6qqaRXbyn+dRQHNgGOAo4DJZnZY8NdDnaulPj0O3BN8fA/wAIF/rHXuUPtjZvHAHwkcovBdLX0+OOf+CPzRzP4AjAX+XMuh1kht9Sf4Xn8ESoCXazPGA1Gb/akPGlxSIDA62u6c61t5o5lFAvOCD6cQ+JKsPKRNBdYH72cBbwaTwNdmVkagNspmLwPfj0Puk3NuU6XXPQW852XA1TjU/hwOdAYWBf+RpwLzzWygc26jx7FXpTb+5ip7BXgfn5ICtdQfM7scOAs4ya8fVEG1/fmEN78nNeriBnSi0qQSMBMYGbxvQJ99vG4ugdFA+aTSmcHt1wF3B++nAZkEr/kI4z61qdTmFmBSOPdnjzZrqcOJZo8+n66V2twIvB7m/TkdWAYk12U/vP57ox5MNPseQB18+K8CG4BiAr/wrybwK3IqsCj4h3nXPl6bDiwBvgceK//iB2KAl4LPzQdOrAd9ehH4BlhM4FdRm3Duzx5t6jQpePT5vBHcvphALZt2Yd6fVQR+TC0M3urybCov+nNO8L0KgU3AtLrqT23fdEWziIhUaKhnH4mISBWUFEREpIKSgoiIVFBSEBGRCkoKIiJSQUlBwp6Z7azj/U00sx619F6lwcqnS8zs3eqqhZpZkpn9ujb2LVIVnZIqYc/MdjrnmtTi+0W5n4q1eapy7Gb2PPCtc+6v+2nfCXjPOderLuKThkcjBamXzCzZzN4ws7nB2+Dg9oFmNtPMFgT/2y24/Qoze83M3gU+MrMTzOxzM3s9WPf/5Uq18z8vr5lvZjuDheoWmdlsM2sV3H548PFcM7u7hqOZWfxU0K+JmX1iZvMtUL9/RLDN34DDg6OLfwbb3hbcz2Iz+79a/N8oDZCSgtRXDwMPOeeOAs4DJga3rwCGOOf6Eag0el+l1wwCLnfOnRh83A+4GegBHAYMrmI/jYHZzrk+wJfAtZX2/3Bw/9XWxwnW2TmJwNXkAAXAOc65/gTW73ggmJRuB753zvV1zt1mZqcCXYGBQF9ggJkNqW5/IvvSEAviScNwMtCjUtXLRDNLAJoCz5tZVwIVLqMrveZ/zrnKdfa/ds5lAZjZQgL1cqbvsZ8ifioeOA84JXh/ED+t7fAK8K99xBlX6b3nAf8LbjfgvuAXfBmBEUSrKl5/avC2IPi4CYEk8eU+9ieyX0oKUl9FAIOcc/mVN5rZo8BnzrlzgsfnP6/09K493qOw0v1Sqv73Uux+mpjbV5v9yXfO9TWzpgSSyw3AIwTWTEgGBjjnis1sLRBbxesNuN859+QB7lekSjp8JPXVRwTWHADAzMrLIjcFfgzev8LD/c8mcNgK4KLqGjvndhBYZvO3ZhZNIM7sYEIYBnQMNs0DEiq9dBpwVXBNAMysnZml1FIfpAFSUpD6IN7MsirdbiXwBZsenHxdRqDcOcA/gPvNbAYQ6WFMNwO3mtnXQBtgR3UvcM4tIFCl8yICi86km1kGgVHDimCbrcCM4Cms/3TOfUTg8NQsM/sGeJ2fJw2RA6JTUkU8EFz9Ld8558zsImCUc25Eda8T8ZvmFES8MQB4LHjG0HZ8WtpU5EBppCAiIhU0pyAiIhWUFEREpIKSgoiIVFBSEBGRCkoKIiJSQUlBREQq/D8V35N4uJOm7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.110415</td>\n",
       "      <td>0.068449</td>\n",
       "      <td>0.966200</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(1, max_lr= 1e-04, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.038121</td>\n",
       "      <td>0.980636</td>\n",
       "      <td>17:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(1, max_lr= 1e-05, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how simple that was?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/databunch_full/models/full_model_learner.pth')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save(\"full_model_learner\", return_path = True)\n",
    "#learner.load(\"./models/small_model.pth\")\n",
    "#PosixPath('data/databunch_small/models/small_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generate predictions. This is where you can get tripped up because the `databunch` does not load data in sorted order. So we'll have to do reorder the generated predictions to match their original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the get_preds method does not yield the elements in order by default\n",
    "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
    "    \"\"\"\n",
    "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    return preds[reverse_sampler, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = get_preds_as_nparray(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate a submission if you like, though you'll probably want to use a different set of configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(DATA_PATH / \"jigsaw_sample_submission.csv\")\n",
    "if config.testing: sample_submission = sample_submission.head(test.shape[0])\n",
    "sample_submission[label_cols] = test_preds\n",
    "sample_submission.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
